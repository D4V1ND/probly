{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb8ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import torch\n",
    "from torch_imp import HistogramBinning\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic binary classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples = 2000,\n",
    "    n_features = 20,\n",
    "    n_informative = 10,\n",
    "    n_redundant = 2,\n",
    "    random_state = 42)\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "# Split into three sets:\n",
    "# Training: 0-1500 (not used in this demo, for illustration)\n",
    "# Calibration: 1500-1750 (used to fit the calibrator)\n",
    "# Test: 1750-2000 (used to evaluate calibration performance)\n",
    "X_train, y_train = X[:1500], y[:1500]\n",
    "X_cal, y_cal = X[1500:1750], y[1500:1750]\n",
    "X_test, y_test = X[1750:], y[1750:]\n",
    "\n",
    "print(\"shapes:\", X_train.shape, X_cal.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef1571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ImageNet weights for demonstration\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c02f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and preprocess CIFAR-10 test set with ImageNet normalization\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]    ),\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fbcedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert CIFAR-10 to binary: \"cat\" (class 3) vs. \"not cat\"\n",
    "import torch\n",
    "\n",
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "           \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "target_class = 3\n",
    "\n",
    "all_images = []\n",
    "binary_labels = []\n",
    "\n",
    "for img, label in dataset:\n",
    "    all_images.append(img)\n",
    "\n",
    "    binary_labels.append(1 if label == target_class else 0)\n",
    "\n",
    "all_images = torch.stack(all_images)\n",
    "binary_labels = torch.tensor(binary_labels)\n",
    "\n",
    "len(all_images), len(binary_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da41f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract softmax probabilities for ImageNet \"tammy cat\" class (281)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()\n",
    "\n",
    "probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, _labels in loader :\n",
    "        imgs = imgs  # noqa: PLW0127, PLW2901\n",
    "\n",
    "        logits = model(imgs)\n",
    "\n",
    "        softmaxed = F.softmax(logits, dim=1)\n",
    "\n",
    "        cat_probs = softmaxed[:, 281]\n",
    "\n",
    "        probs.append(cat_probs)\n",
    "y_pred = torch.cat(probs)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit histogram binnig on calibration set, evaluate on test set\n",
    "device = torch.device(\"cpu\")\n",
    "calibrator = HistogramBinning(base_model=model, device=device)\n",
    "\n",
    "#Fit on CALIBRATION set\n",
    "cal_preds = torch.tensor(y_pred[1500:1750], dtype=torch.float32)\n",
    "cal_labels = torch.tensor(binary_labels[1500:1750], dtype=torch.float32)\n",
    "calibrator.fit(cal_preds, cal_labels)\n",
    "\n",
    "#Evaluate on TEST set\n",
    "test_preds = torch.tensor(y_pred[1750:], dtype=torch.float32)\n",
    "test_labels = torch.tensor(binary_labels[1750:], dtype=torch.float32)\n",
    "\n",
    "test_preds_calibrated = calibrator.predict(test_preds)\n",
    "\n",
    "print(\"Test predictions before calibration (first 10):\")\n",
    "print(test_preds[:10])\n",
    "\n",
    "print(\"\\nTest predictions **after** calibration (first 10):\")\n",
    "print(test_preds_calibrated[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab61adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute ECE and plot reliability diagram on the test set ---\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# Function to compute ECE (Expected Calibration Error)\n",
    "def compute_ece(preds: torch.Tensor, labels: torch.Tensor, n_bins: int = 15) -> float:\n",
    "\n",
    "    bins = torch.linspace(0.0, 1.0, n_bins + 1, device=preds.device)\n",
    "    ece = 0.0\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        start, end = bins[i], bins[i+1]\n",
    "        # select predictions in this bin\n",
    "        in_bin = (preds >= start) & (preds < end)\n",
    "        bin_size = in_bin.sum().item()\n",
    "\n",
    "        if bin_size > 0:\n",
    "            # Convert to float for mean calculation\n",
    "            avg_pred = preds[in_bin].float().mean().item()\n",
    "            avg_label = labels[in_bin].float().mean().item()\n",
    "            # weight by fraction of samples in this bin\n",
    "            ece += (bin_size / len(preds)) * abs(avg_pred - avg_label)\n",
    "\n",
    "    return ece\n",
    "\n",
    "\n",
    "# Compute ECE before & after calibration\n",
    "ece_before = compute_ece(test_preds, test_labels)\n",
    "ece_after  = compute_ece(test_preds_calibrated, test_labels)\n",
    "\n",
    "print(f\"ECE before calibration: {ece_before:.4f}\")\n",
    "print(f\"ECE after calibration:  {ece_after:.4f}\")\n",
    "\n",
    "\n",
    "# Function to plot reliability diagram\n",
    "def reliability_plot(raw: torch.Tensor, calibrated: torch.Tensor, labels: torch.Tensor, n_bins: int = 15) -> None:\n",
    "\n",
    "    bins = torch.linspace(0.0, 1.0, n_bins + 1, device=raw.device)\n",
    "\n",
    "    def bin_stats(preds: torch.Tensor) -> tuple[np.ndarray, np.ndarray]:\n",
    "        conf = []\n",
    "        acc = []\n",
    "        for i in range(n_bins):\n",
    "            in_bin = (preds >= bins[i]) & (preds < bins[i+1])\n",
    "            if in_bin.sum() > 0:\n",
    "                conf.append(preds[in_bin].float().mean().item())\n",
    "                acc.append(labels[in_bin].float().mean().item())\n",
    "            else:\n",
    "                conf.append(0.0)\n",
    "                acc.append(0.0)\n",
    "        return np.array(conf), np.array(acc)\n",
    "\n",
    "    raw_conf, raw_acc = bin_stats(raw)\n",
    "    cal_conf, cal_acc = bin_stats(calibrated)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot([0, 1], [0, 1], \"--\", label=\"Perfect Calibration\")\n",
    "    plt.plot(raw_conf, raw_acc, label=\"Before Calibration\")\n",
    "    plt.plot(cal_conf, cal_acc, label=\"After Calibration\")\n",
    "    plt.xlabel(\"Average predicted probability\")\n",
    "    plt.ylabel(\"Accuracy in bin\")\n",
    "    plt.title(\"Reliability Diagram\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Draw the plot using test set\n",
    "reliability_plot(test_preds, test_preds_calibrated, test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
